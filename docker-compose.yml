version: '3.9'

services:
  postgres:
    image: postgres:15-alpine
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8"
    ports:
      - "5432:5432"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  localstack:
    container_name: localstack_persist
    image: gresau/localstack-persist:4
    ports:
      - "4566:4566"
    environment:
      # Bật persistence cho S3
      - PERSIST_S3=1
      - PERSIST_FORMAT=json
      - PERSIST_FREQUENCY=10  # Lưu mỗi 10 giây
      - DEBUG=1
      # Các service cần dùng
      - SERVICES=s3
    volumes:
      - "./localstack-data:/persisted-data"  # Path mới chính xác
      - "/var/run/docker.sock:/var/run/docker.sock"
    restart: unless-stopped

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: WARN
    ports:
      - "2181:2181"
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | timeout 5 bash -c 'cat > /dev/tcp/localhost/2181' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"  # Port dùng cho các container nội bộ (Airflow, FastAPI, Spark,...)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT_INTERNAL://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT_INTERNAL://0.0.0.0:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
    volumes:
      - kafka-data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server kafka:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped



  airflow-init:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow_init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _PIP_ADDITIONAL_REQUIREMENTS=google-api-python-client boto3 kafka-python
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: >
      bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true"
    restart: "no"

  airflow-webserver:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow_webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _PIP_ADDITIONAL_REQUIREMENTS=google-api-python-client boto3 kafka-python
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command: airflow webserver
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  airflow-scheduler:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow_scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _PIP_ADDITIONAL_REQUIREMENTS=google-api-python-client boto3 kafka-python
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: airflow scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark_master
    depends_on:
      kafka:
        condition: service_healthy
      localstack:
        condition: service_healthy
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
    ports:
      - "7077:7077"
      - "8081:8080"
    volumes:
      - ./spark:/opt/spark/scripts
    restart: unless-stopped

  spark-worker:
    image: bitnami/spark:3.5.0
    container_name: spark_worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    ports:
      - "8082:8081"
    volumes:
      - ./spark:/opt/spark/scripts
    restart: unless-stopped
  
  # Jupyter Notebook cho Analysis
  jupyter:
    container_name: youtube_jupyter
    image: jupyter/all-spark-notebook  # IMAGE ĐẦY ĐỦ JAVA + SPARK
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=youtube123
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - AWS_DEFAULT_REGION=us-east-1
      - S3_ENDPOINT_URL=http://localstack:4566
      - PYSPARK_SUBMIT_ARGS=--jars local:/home/jovyan/spark_jars/hadoop-aws-3.3.2.jar,local:/home/jovyan/spark_jars/aws-java-sdk-bundle-1.11.1026.jar pyspark-shell
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/home/jovyan/data
      - ./spark_jars:/home/jovyan/spark_jars 
    depends_on:
      - localstack
    restart: always
    entrypoint: >
      /bin/bash -c "pip install s3fs boto3 google-api-python-client requests && start-notebook.sh --NotebookApp.token=youtube123"

volumes:
  postgres-db-volume:
  kafka-data:
  zookeeper-data:
  zookeeper-logs:

networks:
  default:
    name: airflow_network
    driver: bridge
